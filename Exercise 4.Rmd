---
title: "Exercise 4"
author: "Sonali Mishra"
date: "05/01/2022"
output:
  md_document:
    variant: markdown_github
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache = TRUE)
# installs the librarian package if you don't have it
if (!("librarian" %in% rownames(utils::installed.packages()))) {
  utils::install.packages("librarian")
}

# put all of the packages that you import here
librarian::shelf( 
  cran_repo = "https://cran.microsoft.com/", # Dallas, TX
  ask = FALSE,
  stats, # https://stackoverflow.com/questions/26935095/r-dplyr-filter-not-masking-base-filter#answer-26935536
  here,
  kableExtra,
  rlang,
  ggthemes,
  tidyverse,
  janitor,
  magrittr,
  glue,
  lubridate,
  haven,
  snakecase,
  sandwich,
  lmtest,
  gganimate,
  gapminder,
  stargazer,
  snakecase,
  rpart,
  rpart.plot,
  rsample,
  randomForest,
  modelr,
  gbm,
  pdp,
  remotes,
  urbnmapr,
  ggmap,
  maps,
  mapdata,
  usmap,
  scales,
  foreach,
  caret,
  mosaic,
  LICORS,
  ggcorrplot,
  arules,
  arulesViz,
  igraph,
  RColorBrewer
)

# tell here where we are so we can use it elsewhere
here::i_am("Data-Mining-PS4/include.R")
```

##Clustering and PCA

The idea is to see if there is any visible appearance of clustering based on color of wine and/or quality of wine. We work with 2 and 10 clusters for wine and quality seperation respectively. We perform both kmeans clustering and hierarchial clustering. But dues to presence of too many outliers hierarchial clustering doesnt provide satisfying results. 
Next we also explore PCA. We start with 2 summaries and that gives us good results for clustering based on color of wine. To get better results for clustering based on quality we try with 4 components that cummulatively explain nearly 74% of the variation in the data but individually dont explain much. 

```{r}
#read data and remove NAs
wine = read_csv(here("Data-Mining-PS4/Clustering and PCA/wine.csv"))
wine = na.omit(wine)

#centre, scaling and rescaling
X = wine[, -(12:13)]
X = scale(X, center=TRUE, scale=TRUE)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")

```
Lets look at Kmeans and Kmeans++ clustering. 

KMEANS
Analyse if any clustering occurs with 2 clusters based on wine color

```{r}
#kmeans for wine comparison 
kmeans_cluster_wine = kmeans(X, 2, nstart=25)

ggplot(wine) + 
  geom_point(aes(color, density, color=factor(kmeans_cluster_wine$cluster)))+
  xlab("Color of the wine in original data") +
  ylab("Density") +
  labs(color = "Kmeans clustering") +
  labs(title = "Kmeans clustering showing seperation based on color of wine")
```

Analyse if any clustering occurs with 10 clusters based on wine quality

```{r}

#kmeans for quality comparison
kmeans_cluster_quality = kmeans(X, 10, nstart=25)

ggplot(wine) + 
  geom_point(aes(quality, density, color=factor(kmeans_cluster_quality$cluster)))+
  xlab("Quality of the wine in original data") +
  ylab("Density") +
  labs(color = "Kmeans clustering")+
  labs(title = "Kmeans clustering showing seperation based on quality of wine") +
  scale_color_brewer(palette = "Dark2")

```

KMEANS++
Analyse if any clustering occurs with 2 clusters based on wine color

```{r}

#kmeans++ for wine comparison
kmeanspp_cluster_wine = kmeanspp(X, 2, nstart=25)

ggplot(wine) + 
  geom_point(aes(color, density, color=factor(kmeanspp_cluster_wine$cluster)))+
  xlab("Color of the wine in original data") +
  ylab("Density") +
  labs(color = "Kmeans++ clustering") +
  labs(title = "Kmeans++ clustering showing seperation based on color of wine")
```

Analyse if any clustering occurs with 10 clusters based on wine quality

```{r}
#kmeans++ for quality comparison
kmeanspp_cluster_quality = kmeanspp(X, 10, nstart=25)

ggplot(wine) + 
  geom_point(aes(quality, density, color=factor(kmeanspp_cluster_quality$cluster)))+
  xlab("Quality of the wine in original data") +
  ylab("Density") +
  labs(color = "Kmeans++ clustering")+
  labs(title = "Kmeans++ clustering showing seperation based on quality of wine") +
  scale_color_brewer(palette = "Dark2")
```
Both kmeans and kmeans++ provide good results for clustering for wine color but neither does that well for quality based clustering. 
If we have to pick one of these two models we will use within and bewtween clusters to check which is better. 

```{r}

#tabular representation
df = data.frame(
  kmeanserr = c(kmeans_cluster_wine$tot.withinss,kmeans_cluster_wine$betweenss,kmeans_cluster_quality$tot.withinss,kmeans_cluster_quality$betweenss),
  kmeanspperr = c(kmeanspp_cluster_wine$tot.withinss,kmeanspp_cluster_wine$betweenss,kmeanspp_cluster_quality$tot.withinss,kmeanspp_cluster_quality$betweenss)
)
colnames(df) = c("Kmeans", "Kmeans++")
rownames(df) = c("within cluster error for wine","between cluster error for wine","within cluster error for quality","between cluster error for quality")

df = df %>%
  kbl() %>%
  kable_material_dark()
df

```

Clearly Kmeans++ would be our preference as it has a higher between cluster error and lower within clsuter error indicating more homgeneity and seperated clsuters. 

Now we try using PCA analysis
First we use 2 components and see if any indicative clustering occurs for both color and quality

```{r}

pc2 = prcomp(X, scale=TRUE, rank=2)
scores2 = pc2$x

qplot(scores2[,1], scores2[,2], color=wine$color)+
  xlab("Principal Component 1") +
  ylab("Principal Component 2") +
  labs(color = "Wine Color")+
  labs(title = "PCA Analysis for 2 components between PCA1 and PCA2 based on wine")

qplot(scores2[,1], scores2[,2], color=wine$quality)+
  xlab("Principal Component 1") +
  ylab("Principal Component 2") +
  labs(color = "Wine Quality")+
  labs(title = "PCA Analysis for 2 components between PCA1 and PCA2 based on quality") +
  scale_color_gradientn(colours = rainbow(5))

loadings_summary2 = pc2$rotation %>%
  as.data.frame()%>%
  rownames_to_column('features')%>%
  kbl() %>%
  kable_material_dark()
loadings_summary2

summary(pc2)

```

While we can be quite satisfied with clustering based on color, we can increase summaries to improve clustering based on quality. This is also evident from the fact that 2 components explain only 50% of the variation in the data. 

```{r}

pc4 = prcomp(X, scale=TRUE, rank=4)
scores4 = pc4$x

qplot(scores4[,1], scores4[,2], color=wine$quality)+
  xlab("Principal Component 1") +
  ylab("Principal Component 2") +
  labs(color = "Wine Quality")+
  labs(title = "PCA Analysis for 4 components between PCA1 and PCA2 for quality") +
  scale_color_gradientn(colours = rainbow(5))

qplot(scores4[,2], scores4[,3], color=wine$quality)+
  xlab("Principal Component 2") +
  ylab("Principal Component 3") +
  labs(color = "Wine Quality")+
  labs(title = "PCA Analysis for 4 components between PCA2 and PCA3 for quality") +
  scale_color_gradientn(colours = rainbow(5))

qplot(scores4[,3], scores4[,4], color=wine$quality)+
  xlab("Principal Component 3") +
  ylab("Principal Component 4") +
  labs(color = "Wine Quality")+
  labs(title = "PCA Analysis for 4 components between PCA3 and PCA4 for quality") +
  scale_color_gradientn(colours = rainbow(5))

qplot(scores4[,1], scores4[,4], color=wine$quality)+
  xlab("Principal Component 1") +
  ylab("Principal Component 4") +
  labs(color = "Wine Quality")+
  labs(title = "PCA Analysis for 4 components between PCA1 and PCA4 for quality") +
  scale_color_gradientn(colours = rainbow(5))

loadings_summary4 = pc4$rotation %>%
  as.data.frame()%>%
  rownames_to_column('features')%>%
  kbl() %>%
  kable_material_dark()
loadings_summary4

summary(pc4)

```

Not the most impressive result but still much better. 
4 components are able to explain a lot more of the variation but not completely. So if i had to use supervised learning models on PCA i would pick the model with 4 components versus 2 components. 

```{r}

grocery_txt = read.delim(here("Data-Mining-PS4/Association rules for grocery purchases/groceries.txt"))
write.csv(grocery_txt, "C:/Users/hp/Documents/Data-Mining-PS4/Association rules for grocery purchases/groceries.csv", quote = FALSE, row.names = FALSE)
grocery_trans = read.transactions("C:/Users/hp/Documents/Data-Mining-PS4/Association rules for grocery purchases/groceries.csv", format = 'basket', sep=',')

itemFrequencyPlot(grocery_trans,topN=20,type="absolute",col=brewer.pal(8,'Pastel2'), main="Absolute Item Frequency Plot")

groccery_rules = apriori(grocery_trans, parameter = list(supp=0.001, conf=0.1,maxlen=4,minlen=2))
inspect(groccery_rules[1:50])

plot(groccery_rules)
plot(groccery_rules, measure = c("support", "lift"), shading = "confidence")
plot(groccery_rules, method='two-key plot')

sub1 = subset(groccery_rules, subset=confidence > 0.01 & support > 0.01)
saveAsGraph(sub1, file = "groceryrules.graphml")

plot(head(sub1, 100, by='lift'), method='graph')

knitr::include_graphics("C:/Users/hp/Documents/Data-Mining-PS4/groceryrules1.graphml.gephi")
```

