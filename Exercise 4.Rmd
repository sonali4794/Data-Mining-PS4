---
title: "Exercise 4"
author: "Sonali Mishra"
date: "05/01/2022"
output:
  md_document:
    variant: markdown_github
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache = TRUE)
# installs the librarian package if you don't have it
if (!("librarian" %in% rownames(utils::installed.packages()))) {
  utils::install.packages("librarian")
}

# put all of the packages that you import here
librarian::shelf( 
  cran_repo = "https://cran.microsoft.com/", # Dallas, TX
  ask = FALSE,
  stats, # https://stackoverflow.com/questions/26935095/r-dplyr-filter-not-masking-base-filter#answer-26935536
  here,
  kableExtra,
  rlang,
  ggthemes,
  tidyverse,
  janitor,
  magrittr,
  glue,
  lubridate,
  haven,
  snakecase,
  sandwich,
  lmtest,
  gganimate,
  gapminder,
  stargazer,
  snakecase,
  rpart,
  rpart.plot,
  rsample,
  randomForest,
  modelr,
  gbm,
  pdp,
  remotes,
  urbnmapr,
  ggmap,
  maps,
  mapdata,
  usmap,
  scales,
  foreach,
  caret,
  mosaic,
  LICORS,
  ggcorrplot,
  arules,
  arulesViz,
  igraph,
  RColorBrewer,
  parallel,
  iterators,
  doParallel
)

# tell here where we are so we can use it elsewhere
here::i_am("Data-Mining-PS4/include.R")
```

##Clustering and PCA

The idea is to see if there is any visible appearance of clustering based on color of wine and/or quality of wine. We work with 2 and 10 clusters for wine and quality seperation respectively. We perform both kmeans clustering and hierarchial clustering. But dues to presence of too many outliers hierarchial clustering doesnt provide satisfying results. 
Next we also explore PCA. We start with 2 summaries and that gives us good results for clustering based on color of wine. To get better results for clustering based on quality we try with 4 components that cummulatively explain nearly 74% of the variation in the data but individually dont explain much. 

```{r}
#read data and remove NAs
wine = read_csv(here("Data-Mining-PS4/Clustering and PCA/wine.csv"))
wine = na.omit(wine)

#centre, scaling and rescaling
X = wine[, -(12:13)]
X = scale(X, center=TRUE, scale=TRUE)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")

```
Lets look at Kmeans and Kmeans++ clustering. 

KMEANS
Analyse if any clustering occurs with 2 clusters based on wine color

```{r}
#kmeans for wine comparison 
kmeans_cluster_wine = kmeans(X, 2, nstart=25)

ggplot(wine) + 
  geom_point(aes(color, density, color=factor(kmeans_cluster_wine$cluster)))+
  xlab("Color of the wine in original data") +
  ylab("Density") +
  labs(color = "Kmeans clustering") +
  labs(title = "Kmeans clustering showing seperation based on color of wine")
```

Analyse if any clustering occurs with 10 clusters based on wine quality

```{r}

#kmeans for quality comparison
kmeans_cluster_quality = kmeans(X, 10, nstart=25)

ggplot(wine) + 
  geom_point(aes(quality, density, color=factor(kmeans_cluster_quality$cluster)))+
  xlab("Quality of the wine in original data") +
  ylab("Density") +
  labs(color = "Kmeans clustering")+
  labs(title = "Kmeans clustering showing seperation based on quality of wine") +
  scale_color_brewer(palette = "Dark2")

```

KMEANS++
Analyse if any clustering occurs with 2 clusters based on wine color

```{r}

#kmeans++ for wine comparison
kmeanspp_cluster_wine = kmeanspp(X, 2, nstart=25)

ggplot(wine) + 
  geom_point(aes(color, density, color=factor(kmeanspp_cluster_wine$cluster)))+
  xlab("Color of the wine in original data") +
  ylab("Density") +
  labs(color = "Kmeans++ clustering") +
  labs(title = "Kmeans++ clustering showing seperation based on color of wine")
```

Analyse if any clustering occurs with 10 clusters based on wine quality

```{r}
#kmeans++ for quality comparison
kmeanspp_cluster_quality = kmeanspp(X, 10, nstart=25)

ggplot(wine) + 
  geom_point(aes(quality, density, color=factor(kmeanspp_cluster_quality$cluster)))+
  xlab("Quality of the wine in original data") +
  ylab("Density") +
  labs(color = "Kmeans++ clustering")+
  labs(title = "Kmeans++ clustering showing seperation based on quality of wine") +
  scale_color_brewer(palette = "Dark2")
```
Both kmeans and kmeans++ provide good results for clustering for wine color but neither does that well for quality based clustering. 
If we have to pick one of these two models we will use within and bewtween clusters to check which is better. 

```{r}

#tabular representation
df = data.frame(
  kmeanserr = c(kmeans_cluster_wine$tot.withinss,kmeans_cluster_wine$betweenss,kmeans_cluster_quality$tot.withinss,kmeans_cluster_quality$betweenss),
  kmeanspperr = c(kmeanspp_cluster_wine$tot.withinss,kmeanspp_cluster_wine$betweenss,kmeanspp_cluster_quality$tot.withinss,kmeanspp_cluster_quality$betweenss)
)
colnames(df) = c("Kmeans", "Kmeans++")
rownames(df) = c("within cluster error for wine","between cluster error for wine","within cluster error for quality","between cluster error for quality")

df = df %>%
  kbl() %>%
  kable_material_dark()
df

```

Clearly Kmeans++ would be our preference as it has a higher between cluster error and lower within clsuter error indicating more homgeneity and seperated clsuters. 

Now we try using PCA analysis
First we use 2 components and see if any indicative clustering occurs for both color and quality

```{r}

pc2 = prcomp(X, scale=TRUE, rank=2)
scores2 = pc2$x

qplot(scores2[,1], scores2[,2], color=wine$color)+
  xlab("Principal Component 1") +
  ylab("Principal Component 2") +
  labs(color = "Wine Color")+
  labs(title = "PCA Analysis for 2 components between PCA1 and PCA2 based on wine")

qplot(scores2[,1], scores2[,2], color=wine$quality)+
  xlab("Principal Component 1") +
  ylab("Principal Component 2") +
  labs(color = "Wine Quality")+
  labs(title = "PCA Analysis for 2 components between PCA1 and PCA2 based on quality") +
  scale_color_gradientn(colours = rainbow(5))

loadings_summary2 = pc2$rotation %>%
  as.data.frame()%>%
  rownames_to_column('features')%>%
  kbl() %>%
  kable_material_dark()
loadings_summary2

summary(pc2)

```

While we can be quite satisfied with clustering based on color, we can increase summaries to improve clustering based on quality. This is also evident from the fact that 2 components explain only 50% of the variation in the data. 

```{r}

pc4 = prcomp(X, scale=TRUE, rank=4)
scores4 = pc4$x

qplot(scores4[,1], scores4[,2], color=wine$quality)+
  xlab("Principal Component 1") +
  ylab("Principal Component 2") +
  labs(color = "Wine Quality")+
  labs(title = "PCA Analysis for 4 components between PCA1 and PCA2 for quality") +
  scale_color_gradientn(colours = rainbow(5))

qplot(scores4[,2], scores4[,3], color=wine$quality)+
  xlab("Principal Component 2") +
  ylab("Principal Component 3") +
  labs(color = "Wine Quality")+
  labs(title = "PCA Analysis for 4 components between PCA2 and PCA3 for quality") +
  scale_color_gradientn(colours = rainbow(5))

qplot(scores4[,3], scores4[,4], color=wine$quality)+
  xlab("Principal Component 3") +
  ylab("Principal Component 4") +
  labs(color = "Wine Quality")+
  labs(title = "PCA Analysis for 4 components between PCA3 and PCA4 for quality") +
  scale_color_gradientn(colours = rainbow(5))

qplot(scores4[,1], scores4[,4], color=wine$quality)+
  xlab("Principal Component 1") +
  ylab("Principal Component 4") +
  labs(color = "Wine Quality")+
  labs(title = "PCA Analysis for 4 components between PCA1 and PCA4 for quality") +
  scale_color_gradientn(colours = rainbow(5))

loadings_summary4 = pc4$rotation %>%
  as.data.frame()%>%
  rownames_to_column('features')%>%
  kbl() %>%
  kable_material_dark()
loadings_summary4

summary(pc4)

```

Not the most impressive result but still much better. 
4 components are able to explain a lot more of the variation but not completely. So if i had to use supervised learning models on PCA i would pick the model with 4 components versus 2 components. 

##Market Segmentation

This is a common use case in machine learning and analysing this data will give interesting insights into the market segments preffered by followers of the brand to NutritionH20. If we notice some significant correlation between these segments (my guess is that we will) then it may make sense to perform PCA and reduce the segments from 36 to whatever seems suitable. 

```{r}
#read and remove NAs and scale and center and convert to frequencies from count data 
social_marketing = read_csv(here("Data-Mining-PS4/Market Segmentation/social_marketing.csv"))
social_marketing = na.omit(social_marketing)
L = social_marketing[, -(1)]
L = scale(L, center=TRUE, scale=TRUE)

corr = round(cor(L),1)
ggcorrplot(corr, hc.order = TRUE, outline.color = "white")

```
I do require a optimal number of clusters and I think computing from the elbow plot might be a good starting point for the same. 

```{r}

k_grid = seq(2,30,by=1)
SSE_grid = foreach(k = k_grid, .combine = 'c') %do% {
  cluster_k = kmeans(L, k, nstart = 30)
  cluster_k$tot.withinss
}

plot(k_grid, SSE_grid, main = "Elbow Plot")
```

From this plot I would start with 15 clusters.

```{r}

kmeanspp_cluster = kmeanspp(L, 15, nstart=35)

ggplot(social_marketing) + 
  geom_point(aes(shopping, beauty, color=factor(kmeanspp_cluster$cluster)))+
  xlab("Beauty") +
  ylab("Shopping") +
  labs(color = "Kmeans++ clustering")

```
This is just to get a pictorial view of 15 clustering of the data. I have taken two highly correlated market segments. 

Next we do PCA on this

```{r}

pc1 = prcomp(L, scale=TRUE, rank=15)
loadings_summary = pc1$rotation %>%
  as.data.frame()%>%
  rownames_to_column('market_segments')

ggplot(loadings_summary) +geom_col(aes(x=reorder(market_segments, PC1),y=PC1))+xlab("PC1")+ylab("Market Segments")+coord_flip()

ggplot(loadings_summary) +geom_col(aes(x=reorder(market_segments, PC2),y=PC2))+xlab("PC2")+ylab("Market Segments")+coord_flip()

ggplot(loadings_summary) +geom_col(aes(x=reorder(market_segments, PC3),y=PC3))+xlab("PC3")+ylab("Market Segments")+coord_flip()

ggplot(loadings_summary) +geom_col(aes(x=reorder(market_segments, PC4),y=PC4))+xlab("PC4")+ylab("Market Segments")+coord_flip()

ggplot(loadings_summary) +geom_col(aes(x=reorder(market_segments, PC5),y=PC5))+xlab("PC5")+ylab("Market Segments")+coord_flip()

ggplot(loadings_summary) +geom_col(aes(x=reorder(market_segments, PC6),y=PC6))+xlab("PC6")+ylab("Market Segments")+coord_flip()

ggplot(loadings_summary) +geom_col(aes(x=reorder(market_segments, PC7),y=PC7))+xlab("PC7")+ylab("Market Segments")+coord_flip()

ggplot(loadings_summary) +geom_col(aes(x=reorder(market_segments, PC8),y=PC8))+xlab("PC8")+ylab("Market Segments")+coord_flip()

ggplot(loadings_summary) +geom_col(aes(x=reorder(market_segments, PC9),y=PC9))+xlab("PC9")+ylab("Market Segments")+coord_flip()

ggplot(loadings_summary) +geom_col(aes(x=reorder(market_segments, PC10),y=PC10))+xlab("PC10")+ylab("Market Segments")+coord_flip()

ggplot(loadings_summary) +geom_col(aes(x=reorder(market_segments, PC11),y=PC11))+xlab("PC11")+ylab("Market Segments")+coord_flip()

ggplot(loadings_summary) +geom_col(aes(x=reorder(market_segments, PC12),y=PC12))+xlab("PC12")+ylab("Market Segments")+coord_flip()

ggplot(loadings_summary) +geom_col(aes(x=reorder(market_segments, PC13),y=PC13))+xlab("PC13")+ylab("Market Segments")+coord_flip()

ggplot(loadings_summary) +geom_col(aes(x=reorder(market_segments, PC14),y=PC14))+xlab("PC14")+ylab("Market Segments")+coord_flip()

ggplot(loadings_summary) +geom_col(aes(x=reorder(market_segments, PC15),y=PC15))+xlab("PC15")+ylab("Market Segments")+coord_flip()

df1 = pc1$rotation

df1 = df1 %>%
  kbl() %>%
  kable_material_dark()
df1

summary(pc1)

```

15 principal components explain nearly 73% of the variation in data. 

Lets look a little deeper into some of the graphs. 
PC1 seems to be like an total count of all features as it weighs everything on one side. There is no contrasting here. Most of NutritionH20 followers are heavy participants in religion, food, parenting, sports and school. PC2 seems to contrast between serious segments like religion, news, family versus casual segments like fitness, dating, shopping (entertainment related). PC4 seems to contrast betwwen food and health versus everything else. This is quite interesting partition. It is interesting to note that adult and spam categories are mostly in middle of the spectrum, they are rarely in extreme of the dataset. That maybe explained by the fact that these were mostly filtered out and what remained may not have been significant for PCA to study much about it. 
Since most of the components individually explain very little of the data their partitioning resembles each other quite a bit. For example PC9 contrasts entertainment segments similar to PC2. 

If we were to run supervised learning models, we would benefit from using these 15 components instead of all 36 (reduced by more than half) while losing roghly 25% of the data. 

##Association Rules

Here we look at multiple ways in which shopping baskets of consumers can be connected and analyse through lift parameter. 

```{r}
grocery_txt = read.delim(here("Data-Mining-PS4/Association rules for grocery purchases/groceries.txt"))
write.csv(grocery_txt, "C:/Users/hp/Documents/Data-Mining-PS4/Association rules for grocery purchases/groceries.csv", quote = FALSE, row.names = FALSE)
grocery_trans = read.transactions("C:/Users/hp/Documents/Data-Mining-PS4/Association rules for grocery purchases/groceries.csv", format = 'basket', sep=',')

itemFrequencyPlot(grocery_trans,topN=20,type="absolute",col=brewer.pal(8,'Pastel2'), main="Absolute Item Frequency Plot")

```

This gives a view of most popular items in shopping baskets for grocery shopping

```{r}
groccery_rules = apriori(grocery_trans, parameter = list(supp=0.001, conf=0.1,maxlen=4,minlen=2))

plot(groccery_rules)
plot(groccery_rules, measure = c("support", "lift"), shading = "confidence")
plot(groccery_rules, method='two-key plot')

```

As expected higher confidence leads to lower lift. There is a clear inverse relation between the two. We have chosen support threshold of 0.001 and confidence threshold of 0.1 with highest transaction level at 4 and lowest at 1. Basically I have removed the entries where it reports a transaction with itself for purpose of this exercise. 
Support and lift also have an inverse relation which is understandanble. If something has higher support which means they are bought frequently irrespective of any complement so its associated lift is likely to be low. The Two-way plot shows that as the number of transaction levels increases their confidence is higher and support is lower. Probability of buying something conditioned on 3 products is less than probability of buying something conditioned on 1 so this makes sense. 

```{r}
df2 = arules::DATAFRAME(groccery_rules)
df3 = df2 %>%
  as.data.frame() %>%
  arrange(desc(lift)) 
df4 = head(df3, 20) %>%
  kbl()%>%
  kable_material_dark()
df4

```

This is a quite insightful table. Lets look at an example. Probability of buying ham, white bread and processed cheese is 0.1%. But probability of buying processed cheese given you have bought ham and white bread is 38%. Buying ham, white bread and processed cheese occurred 22 times more frequently than you would expect purchase of ham and white bread and purchase of processed cheese to occur independently. This tables list top 20 lift data in the association rules. These are all very conceivable associations. Evidently the table lists complements. 

```{r}
df5 = df2 %>%
  as.data.frame() %>%
  arrange(lift) 
df6 = head(df5, 20) %>%
  kbl()%>%
  kable_material_dark()
df6
```
Now lets look at lowest 20 lift rules. Some of these look like supplements like curd, milk, yogurt and soda while some others just look like bad choices to go with each other infact so incompatible that people are more likely to buy them independently than to buy them together. 

```{r}
sub1 = subset(groccery_rules, subset=confidence > 0.01 & support > 0.01)
saveAsGraph(sub1, file = "groceryrules.graphml")

knitr::include_graphics("C:/Users/hp/Documents/Data-Mining-PS4/screenshot1.png")
knitr::include_graphics("C:/Users/hp/Documents/Data-Mining-PS4/screenshot2.png")

```

![Gephi graph to the association rules for confidence and support over 0.01](C:/Users/hp/Documents/Data-Mining-PS4/screenshot1.png)